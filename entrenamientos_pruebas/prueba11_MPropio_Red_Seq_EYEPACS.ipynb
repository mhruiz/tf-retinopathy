{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sorted-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.custom_metrics as metrics\n",
    "import lib.evaluation as ev\n",
    "import lib.plotting as plot\n",
    "import lib.models as models\n",
    "import lib.dataset as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imgaug.augmenters as iaa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corrected-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neutral-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_LEVELS_PER_CLASS = [[0], [1,2,3,4]]\n",
    "\n",
    "IMAGE_SIZE = (540, 540, 3)\n",
    "\n",
    "# Specify dataset files\n",
    "TRAIN_FILE = 'DATASET-TRAIN-80.csv'\n",
    "VALIDATION_FILE = 'DATASET-VALIDATION-10_BALANCED.csv'\n",
    "\n",
    "ONE_HOT_FORMAT = False\n",
    "\n",
    "TRAINING_BATCH_SIZE = 32\n",
    "TRAINING_DATA_AUG = True\n",
    "TRAINING_BALANCED = False\n",
    "TRAINING_PREFETCH = 20\n",
    "TRAINING_TAKE_SIZE = None\n",
    "\n",
    "VALIDATION_BATCH_SIZE = 32\n",
    "VALIDATION_DATA_AUG = False\n",
    "VALIDATION_BALANCED = False\n",
    "VALIDATION_PREFETCH = 1\n",
    "VALIDATION_TAKE_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acting-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se aplica data aug\n"
     ]
    }
   ],
   "source": [
    "reload(dt)\n",
    "\n",
    "'''\n",
    "def create_dataset_new(csv_file, \n",
    "                        list_list_classes,\n",
    "                        balanced=False,\n",
    "                        apply_data_augmentation=False,\n",
    "                        batch_size=1, \n",
    "                        prefetch_buffer=None, \n",
    "                        shuffle=False,\n",
    "                        size=None):\n",
    "'''\n",
    "\n",
    "train_dataset, y_true_train = dt.create_dataset_new(TRAIN_FILE, \n",
    "                                                    DR_LEVELS_PER_CLASS, \n",
    "                                                    balanced=TRAINING_BALANCED, \n",
    "                                                    apply_data_augmentation=TRAINING_DATA_AUG, \n",
    "                                                    batch_size=TRAINING_BATCH_SIZE, \n",
    "                                                    prefetch_buffer=TRAINING_PREFETCH, \n",
    "                                                    one_hot_format=ONE_HOT_FORMAT,\n",
    "                                                    size=TRAINING_TAKE_SIZE)\n",
    "\n",
    "val_dataset, y_true_val = dt.create_dataset_new(VALIDATION_FILE, \n",
    "                                                DR_LEVELS_PER_CLASS, \n",
    "                                                balanced=VALIDATION_BALANCED,\n",
    "                                                apply_data_augmentation=VALIDATION_DATA_AUG,\n",
    "                                                batch_size=VALIDATION_BATCH_SIZE,\n",
    "                                                prefetch_buffer=VALIDATION_PREFETCH, \n",
    "                                                is_validation=True,\n",
    "                                                one_hot_format=ONE_HOT_FORMAT,\n",
    "                                                size=VALIDATION_TAKE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-merit",
   "metadata": {},
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "swedish-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6740610690645092, 1: 1.9362775165269543}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_true_train[:,0]), y=y_true_train[:,0])\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "print(d_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "falling-taste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 268, 268, 32)      2432      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 268, 268, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 53, 53, 64)        100416    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        18464     \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 1)         289       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 24, 24, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                13848     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 135,499\n",
      "Trainable params: 135,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32,(5,5), strides=2, input_shape=IMAGE_SIZE),\n",
    "    tf.keras.layers.ReLU(), # 268, 268\n",
    "    tf.keras.layers.Conv2D(64,(7,7), strides=5),\n",
    "    tf.keras.layers.ReLU(), # 53, 53\n",
    "    tf.keras.layers.Conv2D(32,(3,3), strides=2),\n",
    "    tf.keras.layers.ReLU(), # 26, 26\n",
    "    tf.keras.layers.Conv2D(1,(3,3), strides=1),\n",
    "    tf.keras.layers.ReLU(), # 24, 24\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(len(DR_LEVELS_PER_CLASS), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Create callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard('logs/MPropio_RedSeq_EYEPACS', histogram_freq=1, write_graph=False)\n",
    "\n",
    "cbacks = [tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interested-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1776/1776 [==============================] - 209s 118ms/step - loss: 0.6932 - accuracy: 0.4328 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 2/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6926 - accuracy: 0.5294 - val_loss: 0.6911 - val_accuracy: 0.5409\n",
      "Epoch 3/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6911 - accuracy: 0.5288 - val_loss: 0.6896 - val_accuracy: 0.5306\n",
      "Epoch 4/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6896 - accuracy: 0.5320 - val_loss: 0.6862 - val_accuracy: 0.5503\n",
      "Epoch 5/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6879 - accuracy: 0.5430 - val_loss: 0.6831 - val_accuracy: 0.5564\n",
      "Epoch 6/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6863 - accuracy: 0.5470 - val_loss: 0.6835 - val_accuracy: 0.5559\n",
      "Epoch 7/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6846 - accuracy: 0.5607 - val_loss: 0.6808 - val_accuracy: 0.5714\n",
      "Epoch 8/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6836 - accuracy: 0.5613 - val_loss: 0.6902 - val_accuracy: 0.5372\n",
      "Epoch 9/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6829 - accuracy: 0.5634 - val_loss: 0.6785 - val_accuracy: 0.5720\n",
      "Epoch 10/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6820 - accuracy: 0.5629 - val_loss: 0.6788 - val_accuracy: 0.5759\n",
      "Epoch 11/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6809 - accuracy: 0.5720 - val_loss: 0.6786 - val_accuracy: 0.5673\n",
      "Epoch 12/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6797 - accuracy: 0.5695 - val_loss: 0.6761 - val_accuracy: 0.5664\n",
      "Epoch 13/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6796 - accuracy: 0.5706 - val_loss: 0.6750 - val_accuracy: 0.5709\n",
      "Epoch 14/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6790 - accuracy: 0.5775 - val_loss: 0.6798 - val_accuracy: 0.5648\n",
      "Epoch 15/2000\n",
      "1776/1776 [==============================] - 203s 114ms/step - loss: 0.6778 - accuracy: 0.5755 - val_loss: 0.6769 - val_accuracy: 0.5717\n",
      "Epoch 16/2000\n",
      "1776/1776 [==============================] - 196s 110ms/step - loss: 0.6778 - accuracy: 0.5728 - val_loss: 0.6761 - val_accuracy: 0.5737\n",
      "Epoch 17/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6769 - accuracy: 0.5733 - val_loss: 0.6810 - val_accuracy: 0.5686\n",
      "Epoch 18/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6771 - accuracy: 0.5745 - val_loss: 0.6730 - val_accuracy: 0.5895\n",
      "Epoch 19/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6758 - accuracy: 0.5759 - val_loss: 0.6728 - val_accuracy: 0.5837\n",
      "Epoch 20/2000\n",
      "1776/1776 [==============================] - 203s 114ms/step - loss: 0.6760 - accuracy: 0.5776 - val_loss: 0.6777 - val_accuracy: 0.5675\n",
      "Epoch 21/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6746 - accuracy: 0.5828 - val_loss: 0.6737 - val_accuracy: 0.5839\n",
      "Epoch 22/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6745 - accuracy: 0.5786 - val_loss: 0.6700 - val_accuracy: 0.5912\n",
      "Epoch 23/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6743 - accuracy: 0.5791 - val_loss: 0.6719 - val_accuracy: 0.5887\n",
      "Epoch 24/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6731 - accuracy: 0.5811 - val_loss: 0.6694 - val_accuracy: 0.5881\n",
      "Epoch 25/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6728 - accuracy: 0.5834 - val_loss: 0.6678 - val_accuracy: 0.5926\n",
      "Epoch 26/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6716 - accuracy: 0.5875 - val_loss: 0.6677 - val_accuracy: 0.5912\n",
      "Epoch 27/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6716 - accuracy: 0.5853 - val_loss: 0.6667 - val_accuracy: 0.5964\n",
      "Epoch 28/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6704 - accuracy: 0.5864 - val_loss: 0.6771 - val_accuracy: 0.5731\n",
      "Epoch 29/2000\n",
      "1776/1776 [==============================] - 196s 111ms/step - loss: 0.6707 - accuracy: 0.5886 - val_loss: 0.6706 - val_accuracy: 0.5920\n",
      "Epoch 30/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6694 - accuracy: 0.5888 - val_loss: 0.6657 - val_accuracy: 0.5962\n",
      "Epoch 31/2000\n",
      "1776/1776 [==============================] - 196s 110ms/step - loss: 0.6685 - accuracy: 0.5916 - val_loss: 0.6659 - val_accuracy: 0.5923\n",
      "Epoch 32/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6673 - accuracy: 0.5948 - val_loss: 0.6666 - val_accuracy: 0.5931\n",
      "Epoch 33/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6672 - accuracy: 0.5967 - val_loss: 0.6653 - val_accuracy: 0.5884\n",
      "Epoch 34/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6667 - accuracy: 0.5967 - val_loss: 0.6614 - val_accuracy: 0.6045\n",
      "Epoch 35/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6653 - accuracy: 0.5991 - val_loss: 0.6644 - val_accuracy: 0.6001\n",
      "Epoch 36/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6645 - accuracy: 0.5988 - val_loss: 0.6666 - val_accuracy: 0.5928\n",
      "Epoch 37/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6643 - accuracy: 0.5990 - val_loss: 0.6636 - val_accuracy: 0.6012\n",
      "Epoch 38/2000\n",
      "1776/1776 [==============================] - 203s 114ms/step - loss: 0.6625 - accuracy: 0.6035 - val_loss: 0.6616 - val_accuracy: 0.5951\n",
      "Epoch 39/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6639 - accuracy: 0.6004 - val_loss: 0.6685 - val_accuracy: 0.5901\n",
      "Epoch 40/2000\n",
      "1776/1776 [==============================] - 203s 114ms/step - loss: 0.6625 - accuracy: 0.6032 - val_loss: 0.6669 - val_accuracy: 0.5920\n",
      "Epoch 41/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6611 - accuracy: 0.6084 - val_loss: 0.6796 - val_accuracy: 0.5862\n",
      "Epoch 42/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6620 - accuracy: 0.6058 - val_loss: 0.6584 - val_accuracy: 0.6089\n",
      "Epoch 43/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6606 - accuracy: 0.6099 - val_loss: 0.6587 - val_accuracy: 0.6098\n",
      "Epoch 44/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6606 - accuracy: 0.6120 - val_loss: 0.6582 - val_accuracy: 0.6087\n",
      "Epoch 45/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6605 - accuracy: 0.6114 - val_loss: 0.6598 - val_accuracy: 0.6031\n",
      "Epoch 46/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6588 - accuracy: 0.6155 - val_loss: 0.6605 - val_accuracy: 0.5987\n",
      "Epoch 47/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6597 - accuracy: 0.6165 - val_loss: 0.6584 - val_accuracy: 0.6103\n",
      "Epoch 48/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6589 - accuracy: 0.6155 - val_loss: 0.6658 - val_accuracy: 0.5931\n",
      "Epoch 49/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6582 - accuracy: 0.6173 - val_loss: 0.6595 - val_accuracy: 0.6020\n",
      "Epoch 50/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6582 - accuracy: 0.6188 - val_loss: 0.6566 - val_accuracy: 0.6109\n",
      "Epoch 51/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6569 - accuracy: 0.6214 - val_loss: 0.6586 - val_accuracy: 0.6053\n",
      "Epoch 52/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6569 - accuracy: 0.6210 - val_loss: 0.6617 - val_accuracy: 0.6012\n",
      "Epoch 53/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6564 - accuracy: 0.6258 - val_loss: 0.6577 - val_accuracy: 0.6142\n",
      "Epoch 54/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6552 - accuracy: 0.6236 - val_loss: 0.6536 - val_accuracy: 0.6192\n",
      "Epoch 55/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6556 - accuracy: 0.6245 - val_loss: 0.6598 - val_accuracy: 0.6048\n",
      "Epoch 56/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6551 - accuracy: 0.6222 - val_loss: 0.6585 - val_accuracy: 0.6145\n",
      "Epoch 57/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6540 - accuracy: 0.6285 - val_loss: 0.6595 - val_accuracy: 0.6106\n",
      "Epoch 58/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6540 - accuracy: 0.6266 - val_loss: 0.6598 - val_accuracy: 0.5987\n",
      "Epoch 59/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6538 - accuracy: 0.6275 - val_loss: 0.6612 - val_accuracy: 0.6042\n",
      "Epoch 60/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6535 - accuracy: 0.6276 - val_loss: 0.6653 - val_accuracy: 0.6020\n",
      "Epoch 61/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6528 - accuracy: 0.6307 - val_loss: 0.6606 - val_accuracy: 0.5970\n",
      "Epoch 62/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6533 - accuracy: 0.6289 - val_loss: 0.6577 - val_accuracy: 0.6187\n",
      "Epoch 63/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6529 - accuracy: 0.6310 - val_loss: 0.6553 - val_accuracy: 0.6051\n",
      "Epoch 64/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6517 - accuracy: 0.6320 - val_loss: 0.6626 - val_accuracy: 0.5987\n",
      "Epoch 65/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6531 - accuracy: 0.6298 - val_loss: 0.6642 - val_accuracy: 0.6089\n",
      "Epoch 66/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6516 - accuracy: 0.6327 - val_loss: 0.6674 - val_accuracy: 0.5992\n",
      "Epoch 67/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6509 - accuracy: 0.6311 - val_loss: 0.6585 - val_accuracy: 0.6120\n",
      "Epoch 68/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6514 - accuracy: 0.6349 - val_loss: 0.6564 - val_accuracy: 0.6023\n",
      "Epoch 69/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6521 - accuracy: 0.6307 - val_loss: 0.6541 - val_accuracy: 0.6217\n",
      "Epoch 70/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6512 - accuracy: 0.6320 - val_loss: 0.6635 - val_accuracy: 0.5939\n",
      "Epoch 71/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6500 - accuracy: 0.6372 - val_loss: 0.6567 - val_accuracy: 0.6153\n",
      "Epoch 72/2000\n",
      "1776/1776 [==============================] - 196s 111ms/step - loss: 0.6495 - accuracy: 0.6386 - val_loss: 0.6560 - val_accuracy: 0.6103\n",
      "Epoch 73/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6497 - accuracy: 0.6382 - val_loss: 0.6608 - val_accuracy: 0.6012\n",
      "Epoch 74/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6499 - accuracy: 0.6370 - val_loss: 0.6548 - val_accuracy: 0.6073\n",
      "Epoch 75/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6493 - accuracy: 0.6362 - val_loss: 0.6547 - val_accuracy: 0.6128\n",
      "Epoch 76/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6493 - accuracy: 0.6383 - val_loss: 0.6544 - val_accuracy: 0.6081\n",
      "Epoch 77/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6491 - accuracy: 0.6423 - val_loss: 0.6653 - val_accuracy: 0.5989\n",
      "Epoch 78/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6489 - accuracy: 0.6402 - val_loss: 0.6586 - val_accuracy: 0.6089\n",
      "Epoch 79/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6486 - accuracy: 0.6373 - val_loss: 0.6640 - val_accuracy: 0.6006\n",
      "Epoch 80/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6483 - accuracy: 0.6369 - val_loss: 0.6588 - val_accuracy: 0.6003\n",
      "Epoch 81/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6484 - accuracy: 0.6409 - val_loss: 0.6526 - val_accuracy: 0.6101\n",
      "Epoch 82/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6491 - accuracy: 0.6387 - val_loss: 0.6552 - val_accuracy: 0.6209\n",
      "Epoch 83/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6477 - accuracy: 0.6403 - val_loss: 0.6540 - val_accuracy: 0.6134\n",
      "Epoch 84/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6466 - accuracy: 0.6418 - val_loss: 0.6573 - val_accuracy: 0.6181\n",
      "Epoch 85/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6467 - accuracy: 0.6421 - val_loss: 0.6631 - val_accuracy: 0.6095\n",
      "Epoch 86/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6467 - accuracy: 0.6413 - val_loss: 0.6583 - val_accuracy: 0.5992\n",
      "Epoch 87/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6454 - accuracy: 0.6431 - val_loss: 0.6527 - val_accuracy: 0.6153\n",
      "Epoch 88/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6455 - accuracy: 0.6436 - val_loss: 0.6593 - val_accuracy: 0.6042\n",
      "Epoch 89/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6467 - accuracy: 0.6452 - val_loss: 0.6573 - val_accuracy: 0.6098\n",
      "Epoch 90/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6460 - accuracy: 0.6453 - val_loss: 0.6535 - val_accuracy: 0.6167\n",
      "Epoch 91/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6432 - accuracy: 0.6448 - val_loss: 0.6504 - val_accuracy: 0.6145\n",
      "Epoch 92/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6448 - accuracy: 0.6454 - val_loss: 0.6560 - val_accuracy: 0.6067\n",
      "Epoch 93/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6443 - accuracy: 0.6469 - val_loss: 0.6599 - val_accuracy: 0.6067\n",
      "Epoch 94/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6442 - accuracy: 0.6438 - val_loss: 0.6618 - val_accuracy: 0.6051\n",
      "Epoch 95/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6431 - accuracy: 0.6461 - val_loss: 0.6572 - val_accuracy: 0.6165\n",
      "Epoch 96/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6451 - accuracy: 0.6446 - val_loss: 0.6638 - val_accuracy: 0.5976\n",
      "Epoch 97/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6437 - accuracy: 0.6483 - val_loss: 0.6583 - val_accuracy: 0.6078\n",
      "Epoch 98/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6432 - accuracy: 0.6477 - val_loss: 0.6549 - val_accuracy: 0.6062\n",
      "Epoch 99/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6427 - accuracy: 0.6506 - val_loss: 0.6572 - val_accuracy: 0.6103\n",
      "Epoch 100/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6439 - accuracy: 0.6479 - val_loss: 0.6544 - val_accuracy: 0.6120\n",
      "Epoch 101/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6423 - accuracy: 0.6471 - val_loss: 0.6540 - val_accuracy: 0.6159\n",
      "Epoch 102/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6421 - accuracy: 0.6505 - val_loss: 0.6541 - val_accuracy: 0.6142\n",
      "Epoch 103/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6427 - accuracy: 0.6481 - val_loss: 0.6566 - val_accuracy: 0.6101\n",
      "Epoch 104/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6416 - accuracy: 0.6484 - val_loss: 0.6533 - val_accuracy: 0.6148\n",
      "Epoch 105/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6419 - accuracy: 0.6490 - val_loss: 0.6546 - val_accuracy: 0.6181\n",
      "Epoch 106/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6411 - accuracy: 0.6522 - val_loss: 0.6564 - val_accuracy: 0.6153\n",
      "Epoch 107/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6405 - accuracy: 0.6537 - val_loss: 0.6560 - val_accuracy: 0.6142\n",
      "Epoch 108/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6403 - accuracy: 0.6505 - val_loss: 0.6517 - val_accuracy: 0.6120\n",
      "Epoch 109/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6403 - accuracy: 0.6533 - val_loss: 0.6567 - val_accuracy: 0.6076\n",
      "Epoch 110/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6401 - accuracy: 0.6531 - val_loss: 0.6622 - val_accuracy: 0.6051\n",
      "Epoch 111/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6416 - accuracy: 0.6498 - val_loss: 0.6580 - val_accuracy: 0.6131\n",
      "Epoch 112/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6391 - accuracy: 0.6535 - val_loss: 0.6602 - val_accuracy: 0.6051\n",
      "Epoch 113/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6406 - accuracy: 0.6530 - val_loss: 0.6528 - val_accuracy: 0.6123\n",
      "Epoch 114/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6396 - accuracy: 0.6523 - val_loss: 0.6531 - val_accuracy: 0.6140\n",
      "Epoch 115/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6395 - accuracy: 0.6504 - val_loss: 0.6541 - val_accuracy: 0.6159\n",
      "Epoch 116/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6394 - accuracy: 0.6548 - val_loss: 0.6500 - val_accuracy: 0.6187\n",
      "Epoch 117/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6387 - accuracy: 0.6527 - val_loss: 0.6599 - val_accuracy: 0.6137\n",
      "Epoch 118/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6391 - accuracy: 0.6518 - val_loss: 0.6586 - val_accuracy: 0.6078\n",
      "Epoch 119/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6375 - accuracy: 0.6529 - val_loss: 0.6546 - val_accuracy: 0.6234\n",
      "Epoch 120/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6384 - accuracy: 0.6556 - val_loss: 0.6629 - val_accuracy: 0.6014\n",
      "Epoch 121/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6364 - accuracy: 0.6578 - val_loss: 0.6666 - val_accuracy: 0.6062\n",
      "Epoch 122/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6372 - accuracy: 0.6550 - val_loss: 0.6556 - val_accuracy: 0.6156\n",
      "Epoch 123/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6389 - accuracy: 0.6555 - val_loss: 0.6585 - val_accuracy: 0.6095\n",
      "Epoch 124/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6362 - accuracy: 0.6548 - val_loss: 0.6628 - val_accuracy: 0.6064\n",
      "Epoch 125/2000\n",
      "1776/1776 [==============================] - 196s 111ms/step - loss: 0.6367 - accuracy: 0.6574 - val_loss: 0.6539 - val_accuracy: 0.6128\n",
      "Epoch 126/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6368 - accuracy: 0.6523 - val_loss: 0.6579 - val_accuracy: 0.6078\n",
      "Epoch 127/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6389 - accuracy: 0.6538 - val_loss: 0.6656 - val_accuracy: 0.6014\n",
      "Epoch 128/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6367 - accuracy: 0.6571 - val_loss: 0.6630 - val_accuracy: 0.6109\n",
      "Epoch 129/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6355 - accuracy: 0.6574 - val_loss: 0.6641 - val_accuracy: 0.6112\n",
      "Epoch 130/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6369 - accuracy: 0.6560 - val_loss: 0.6538 - val_accuracy: 0.6170\n",
      "Epoch 131/2000\n",
      "1776/1776 [==============================] - 203s 115ms/step - loss: 0.6367 - accuracy: 0.6575 - val_loss: 0.6559 - val_accuracy: 0.6098\n",
      "Epoch 132/2000\n",
      "1776/1776 [==============================] - 195s 110ms/step - loss: 0.6362 - accuracy: 0.6565 - val_loss: 0.6550 - val_accuracy: 0.6165\n",
      "Epoch 133/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6360 - accuracy: 0.6566 - val_loss: 0.6569 - val_accuracy: 0.6162\n",
      "Epoch 134/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6346 - accuracy: 0.6589 - val_loss: 0.6626 - val_accuracy: 0.6028\n",
      "Epoch 135/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6355 - accuracy: 0.6580 - val_loss: 0.6568 - val_accuracy: 0.6176\n",
      "Epoch 136/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6350 - accuracy: 0.6641 - val_loss: 0.6597 - val_accuracy: 0.6064\n",
      "Epoch 137/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6367 - accuracy: 0.6573 - val_loss: 0.6626 - val_accuracy: 0.6073\n",
      "Epoch 138/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6360 - accuracy: 0.6578 - val_loss: 0.6570 - val_accuracy: 0.6170\n",
      "Epoch 139/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6359 - accuracy: 0.6542 - val_loss: 0.6621 - val_accuracy: 0.6140\n",
      "Epoch 140/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6346 - accuracy: 0.6627 - val_loss: 0.6623 - val_accuracy: 0.6134\n",
      "Epoch 141/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6345 - accuracy: 0.6592 - val_loss: 0.6580 - val_accuracy: 0.6048\n",
      "Epoch 142/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6341 - accuracy: 0.6575 - val_loss: 0.6599 - val_accuracy: 0.6237\n",
      "Epoch 143/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6357 - accuracy: 0.6579 - val_loss: 0.6557 - val_accuracy: 0.6201\n",
      "Epoch 144/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6339 - accuracy: 0.6617 - val_loss: 0.6581 - val_accuracy: 0.6126\n",
      "Epoch 145/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6334 - accuracy: 0.6603 - val_loss: 0.6582 - val_accuracy: 0.6134\n",
      "Epoch 146/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6329 - accuracy: 0.6619 - val_loss: 0.6607 - val_accuracy: 0.6167\n",
      "Epoch 147/2000\n",
      "1776/1776 [==============================] - 203s 114ms/step - loss: 0.6347 - accuracy: 0.6600 - val_loss: 0.6593 - val_accuracy: 0.6103\n",
      "Epoch 148/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6338 - accuracy: 0.6577 - val_loss: 0.6534 - val_accuracy: 0.6156\n",
      "Epoch 149/2000\n",
      "1776/1776 [==============================] - 196s 111ms/step - loss: 0.6345 - accuracy: 0.6580 - val_loss: 0.6602 - val_accuracy: 0.6073\n",
      "Epoch 150/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6349 - accuracy: 0.6593 - val_loss: 0.6525 - val_accuracy: 0.6259\n",
      "Epoch 151/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6338 - accuracy: 0.6623 - val_loss: 0.6554 - val_accuracy: 0.6159\n",
      "Epoch 152/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6330 - accuracy: 0.6560 - val_loss: 0.6551 - val_accuracy: 0.6167\n",
      "Epoch 153/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6324 - accuracy: 0.6617 - val_loss: 0.6650 - val_accuracy: 0.6073\n",
      "Epoch 154/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6346 - accuracy: 0.6630 - val_loss: 0.6548 - val_accuracy: 0.6187\n",
      "Epoch 155/2000\n",
      "1776/1776 [==============================] - 202s 113ms/step - loss: 0.6324 - accuracy: 0.6615 - val_loss: 0.6629 - val_accuracy: 0.6151\n",
      "Epoch 156/2000\n",
      "1776/1776 [==============================] - 202s 113ms/step - loss: 0.6331 - accuracy: 0.6611 - val_loss: 0.6602 - val_accuracy: 0.6128\n",
      "Epoch 157/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6342 - accuracy: 0.6610 - val_loss: 0.6597 - val_accuracy: 0.6151\n",
      "Epoch 158/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6340 - accuracy: 0.6581 - val_loss: 0.6652 - val_accuracy: 0.6167\n",
      "Epoch 159/2000\n",
      "1776/1776 [==============================] - 203s 114ms/step - loss: 0.6305 - accuracy: 0.6626 - val_loss: 0.6559 - val_accuracy: 0.6131\n",
      "Epoch 160/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6327 - accuracy: 0.6614 - val_loss: 0.6589 - val_accuracy: 0.6098\n",
      "Epoch 161/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6335 - accuracy: 0.6611 - val_loss: 0.6545 - val_accuracy: 0.6148\n",
      "Epoch 162/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6336 - accuracy: 0.6586 - val_loss: 0.6615 - val_accuracy: 0.6237\n",
      "Epoch 163/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6333 - accuracy: 0.6596 - val_loss: 0.6499 - val_accuracy: 0.6187\n",
      "Epoch 164/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6308 - accuracy: 0.6617 - val_loss: 0.6627 - val_accuracy: 0.6103\n",
      "Epoch 165/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6333 - accuracy: 0.6619 - val_loss: 0.6570 - val_accuracy: 0.6115\n",
      "Epoch 166/2000\n",
      "1776/1776 [==============================] - 205s 115ms/step - loss: 0.6330 - accuracy: 0.6609 - val_loss: 0.6612 - val_accuracy: 0.6181\n",
      "Epoch 167/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6328 - accuracy: 0.6601 - val_loss: 0.6551 - val_accuracy: 0.6148\n",
      "Epoch 168/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6330 - accuracy: 0.6637 - val_loss: 0.6626 - val_accuracy: 0.6176\n",
      "Epoch 169/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6325 - accuracy: 0.6609 - val_loss: 0.6512 - val_accuracy: 0.6234\n",
      "Epoch 170/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6314 - accuracy: 0.6625 - val_loss: 0.6627 - val_accuracy: 0.6153\n",
      "Epoch 171/2000\n",
      "1776/1776 [==============================] - 203s 114ms/step - loss: 0.6329 - accuracy: 0.6587 - val_loss: 0.6582 - val_accuracy: 0.6089\n",
      "Epoch 172/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6310 - accuracy: 0.6624 - val_loss: 0.6610 - val_accuracy: 0.6101\n",
      "Epoch 173/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6302 - accuracy: 0.6615 - val_loss: 0.6606 - val_accuracy: 0.6115\n",
      "Epoch 174/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6311 - accuracy: 0.6621 - val_loss: 0.6599 - val_accuracy: 0.6201\n",
      "Epoch 175/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6299 - accuracy: 0.6592 - val_loss: 0.6704 - val_accuracy: 0.6162\n",
      "Epoch 176/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6312 - accuracy: 0.6590 - val_loss: 0.6515 - val_accuracy: 0.6126\n",
      "Epoch 177/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6319 - accuracy: 0.6606 - val_loss: 0.6579 - val_accuracy: 0.6064\n",
      "Epoch 178/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6320 - accuracy: 0.6585 - val_loss: 0.6631 - val_accuracy: 0.6140\n",
      "Epoch 179/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6298 - accuracy: 0.6615 - val_loss: 0.6587 - val_accuracy: 0.6089\n",
      "Epoch 180/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6285 - accuracy: 0.6648 - val_loss: 0.6667 - val_accuracy: 0.6115\n",
      "Epoch 181/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6294 - accuracy: 0.6636 - val_loss: 0.6557 - val_accuracy: 0.6184\n",
      "Epoch 182/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6288 - accuracy: 0.6622 - val_loss: 0.6639 - val_accuracy: 0.6056\n",
      "Epoch 183/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6297 - accuracy: 0.6599 - val_loss: 0.6561 - val_accuracy: 0.6212\n",
      "Epoch 184/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6301 - accuracy: 0.6577 - val_loss: 0.6642 - val_accuracy: 0.6184\n",
      "Epoch 185/2000\n",
      "1776/1776 [==============================] - 195s 110ms/step - loss: 0.6281 - accuracy: 0.6609 - val_loss: 0.6572 - val_accuracy: 0.6137\n",
      "Epoch 186/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6301 - accuracy: 0.6580 - val_loss: 0.6619 - val_accuracy: 0.6178\n",
      "Epoch 187/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6296 - accuracy: 0.6606 - val_loss: 0.6590 - val_accuracy: 0.6234\n",
      "Epoch 188/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6286 - accuracy: 0.6607 - val_loss: 0.6580 - val_accuracy: 0.6070\n",
      "Epoch 189/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6293 - accuracy: 0.6628 - val_loss: 0.6598 - val_accuracy: 0.6126\n",
      "Epoch 190/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6290 - accuracy: 0.6595 - val_loss: 0.6545 - val_accuracy: 0.6162\n",
      "Epoch 191/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6291 - accuracy: 0.6616 - val_loss: 0.6617 - val_accuracy: 0.6131\n",
      "Epoch 192/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6279 - accuracy: 0.6634 - val_loss: 0.6574 - val_accuracy: 0.6181\n",
      "Epoch 193/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6293 - accuracy: 0.6580 - val_loss: 0.6647 - val_accuracy: 0.6112\n",
      "Epoch 194/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6295 - accuracy: 0.6596 - val_loss: 0.6576 - val_accuracy: 0.6126\n",
      "Epoch 195/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6277 - accuracy: 0.6600 - val_loss: 0.6609 - val_accuracy: 0.6095\n",
      "Epoch 196/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6266 - accuracy: 0.6621 - val_loss: 0.6671 - val_accuracy: 0.6162\n",
      "Epoch 197/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6290 - accuracy: 0.6561 - val_loss: 0.6636 - val_accuracy: 0.6190\n",
      "Epoch 198/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6277 - accuracy: 0.6600 - val_loss: 0.6600 - val_accuracy: 0.6089\n",
      "Epoch 199/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6261 - accuracy: 0.6606 - val_loss: 0.6624 - val_accuracy: 0.6078\n",
      "Epoch 200/2000\n",
      "1776/1776 [==============================] - 197s 111ms/step - loss: 0.6281 - accuracy: 0.6594 - val_loss: 0.6558 - val_accuracy: 0.6142\n",
      "Epoch 201/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6275 - accuracy: 0.6600 - val_loss: 0.6572 - val_accuracy: 0.6167\n",
      "Epoch 202/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6288 - accuracy: 0.6580 - val_loss: 0.6558 - val_accuracy: 0.6181\n",
      "Epoch 203/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6278 - accuracy: 0.6603 - val_loss: 0.6570 - val_accuracy: 0.6095\n",
      "Epoch 204/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6260 - accuracy: 0.6623 - val_loss: 0.6637 - val_accuracy: 0.6117\n",
      "Epoch 205/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6269 - accuracy: 0.6596 - val_loss: 0.6588 - val_accuracy: 0.6162\n",
      "Epoch 206/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6250 - accuracy: 0.6629 - val_loss: 0.6610 - val_accuracy: 0.6159\n",
      "Epoch 207/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6261 - accuracy: 0.6652 - val_loss: 0.6573 - val_accuracy: 0.6237\n",
      "Epoch 208/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6264 - accuracy: 0.6600 - val_loss: 0.6780 - val_accuracy: 0.6051\n",
      "Epoch 209/2000\n",
      "1776/1776 [==============================] - 204s 115ms/step - loss: 0.6262 - accuracy: 0.6606 - val_loss: 0.6640 - val_accuracy: 0.6142\n",
      "Epoch 210/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6271 - accuracy: 0.6611 - val_loss: 0.6547 - val_accuracy: 0.6226\n",
      "Epoch 211/2000\n",
      "1776/1776 [==============================] - 205s 115ms/step - loss: 0.6246 - accuracy: 0.6641 - val_loss: 0.6606 - val_accuracy: 0.6084\n",
      "Epoch 212/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6248 - accuracy: 0.6638 - val_loss: 0.6548 - val_accuracy: 0.6137\n",
      "Epoch 213/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6262 - accuracy: 0.6597 - val_loss: 0.6589 - val_accuracy: 0.6156\n",
      "Epoch 214/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6268 - accuracy: 0.6578 - val_loss: 0.6621 - val_accuracy: 0.6126\n",
      "Epoch 215/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6271 - accuracy: 0.6604 - val_loss: 0.6647 - val_accuracy: 0.6042\n",
      "Epoch 216/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6260 - accuracy: 0.6614 - val_loss: 0.6648 - val_accuracy: 0.6089\n",
      "Epoch 217/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6257 - accuracy: 0.6592 - val_loss: 0.6623 - val_accuracy: 0.6106\n",
      "Epoch 218/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6234 - accuracy: 0.6627 - val_loss: 0.6696 - val_accuracy: 0.6092\n",
      "Epoch 219/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6252 - accuracy: 0.6569 - val_loss: 0.6884 - val_accuracy: 0.5928\n",
      "Epoch 220/2000\n",
      "1776/1776 [==============================] - 195s 110ms/step - loss: 0.6268 - accuracy: 0.6560 - val_loss: 0.6923 - val_accuracy: 0.6034\n",
      "Epoch 221/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6267 - accuracy: 0.6561 - val_loss: 0.6657 - val_accuracy: 0.5987\n",
      "Epoch 222/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6231 - accuracy: 0.6605 - val_loss: 0.6652 - val_accuracy: 0.6073\n",
      "Epoch 223/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6242 - accuracy: 0.6636 - val_loss: 0.6596 - val_accuracy: 0.6073\n",
      "Epoch 224/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6247 - accuracy: 0.6596 - val_loss: 0.6560 - val_accuracy: 0.6142\n",
      "Epoch 225/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6251 - accuracy: 0.6578 - val_loss: 0.6823 - val_accuracy: 0.5962\n",
      "Epoch 226/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6236 - accuracy: 0.6604 - val_loss: 0.6599 - val_accuracy: 0.6220\n",
      "Epoch 227/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6244 - accuracy: 0.6593 - val_loss: 0.6590 - val_accuracy: 0.6087\n",
      "Epoch 228/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6253 - accuracy: 0.6575 - val_loss: 0.6708 - val_accuracy: 0.6087\n",
      "Epoch 229/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6235 - accuracy: 0.6588 - val_loss: 0.6639 - val_accuracy: 0.6217\n",
      "Epoch 230/2000\n",
      "1776/1776 [==============================] - 202s 114ms/step - loss: 0.6227 - accuracy: 0.6610 - val_loss: 0.6609 - val_accuracy: 0.6165\n",
      "Epoch 231/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6226 - accuracy: 0.6633 - val_loss: 0.6589 - val_accuracy: 0.6062\n",
      "Epoch 232/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6250 - accuracy: 0.6560 - val_loss: 0.6556 - val_accuracy: 0.6162\n",
      "Epoch 233/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6238 - accuracy: 0.6599 - val_loss: 0.6570 - val_accuracy: 0.6131\n",
      "Epoch 234/2000\n",
      "1776/1776 [==============================] - 198s 111ms/step - loss: 0.6247 - accuracy: 0.6566 - val_loss: 0.6708 - val_accuracy: 0.6134\n",
      "Epoch 235/2000\n",
      "1776/1776 [==============================] - 200s 112ms/step - loss: 0.6209 - accuracy: 0.6587 - val_loss: 0.6657 - val_accuracy: 0.6137\n",
      "Epoch 236/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6226 - accuracy: 0.6620 - val_loss: 0.6555 - val_accuracy: 0.6170\n",
      "Epoch 237/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6245 - accuracy: 0.6577 - val_loss: 0.6623 - val_accuracy: 0.6073\n",
      "Epoch 238/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6233 - accuracy: 0.6593 - val_loss: 0.6640 - val_accuracy: 0.6167\n",
      "Epoch 239/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6224 - accuracy: 0.6603 - val_loss: 0.6702 - val_accuracy: 0.6073\n",
      "Epoch 240/2000\n",
      "1776/1776 [==============================] - 200s 113ms/step - loss: 0.6222 - accuracy: 0.6619 - val_loss: 0.6613 - val_accuracy: 0.6259\n",
      "Epoch 241/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6246 - accuracy: 0.6572 - val_loss: 0.6732 - val_accuracy: 0.5992\n",
      "Epoch 242/2000\n",
      "1776/1776 [==============================] - 205s 115ms/step - loss: 0.6236 - accuracy: 0.6556 - val_loss: 0.6672 - val_accuracy: 0.6156\n",
      "Epoch 243/2000\n",
      "1776/1776 [==============================] - 202s 113ms/step - loss: 0.6231 - accuracy: 0.6569 - val_loss: 0.6599 - val_accuracy: 0.6092\n",
      "Epoch 244/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6247 - accuracy: 0.6577 - val_loss: 0.6757 - val_accuracy: 0.6081\n",
      "Epoch 245/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6226 - accuracy: 0.6582 - val_loss: 0.6808 - val_accuracy: 0.6089\n",
      "Epoch 246/2000\n",
      "1776/1776 [==============================] - 201s 113ms/step - loss: 0.6225 - accuracy: 0.6606 - val_loss: 0.6661 - val_accuracy: 0.6128\n",
      "Epoch 247/2000\n",
      "1776/1776 [==============================] - 198s 112ms/step - loss: 0.6232 - accuracy: 0.6584 - val_loss: 0.6621 - val_accuracy: 0.6181\n",
      "Epoch 248/2000\n",
      "1776/1776 [==============================] - 199s 112ms/step - loss: 0.6254 - accuracy: 0.6527 - val_loss: 0.6712 - val_accuracy: 0.6059\n",
      "Epoch 249/2000\n",
      "1110/1776 [=================>............] - ETA: 1:12 - loss: 0.6224 - accuracy: 0.6578"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-68e45c13d471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_class_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, clipnorm=1.0), \n",
    "              metrics=['accuracy']) \n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=1, class_weight=d_class_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit6c4dec2e42734bc298ce0c3bfcfccb75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
