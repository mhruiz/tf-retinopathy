{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.custom_callbacks as callbacks\n",
    "import lib.custom_metrics as metrics\n",
    "import lib.evaluation as ev\n",
    "import lib.plotting as plot\n",
    "import lib.models as models\n",
    "import lib.dataset as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imgaug.augmenters as iaa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_LEVELS_PER_CLASS = [[0], [1,2,3,4]]\n",
    "\n",
    "IMAGE_SIZE = (540, 540, 3)\n",
    "\n",
    "# Specify dataset files\n",
    "TRAIN_FILE = 'DATASET-TRAIN-80.csv'\n",
    "VALIDATION_FILE = 'DATASET-VALIDATION-10_BALANCED.csv'\n",
    "\n",
    "ONE_HOT_FORMAT = False\n",
    "\n",
    "TRAINING_BATCH_SIZE = 12\n",
    "TRAINING_DATA_AUG = True\n",
    "TRAINING_BALANCED = False\n",
    "TRAINING_PREFETCH = 20\n",
    "TRAINING_TAKE_SIZE = None\n",
    "\n",
    "VALIDATION_BATCH_SIZE = 12\n",
    "VALIDATION_DATA_AUG = False\n",
    "VALIDATION_BALANCED = False\n",
    "VALIDATION_PREFETCH = 1\n",
    "VALIDATION_TAKE_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(dt)\n",
    "\n",
    "'''\n",
    "def create_dataset_new(csv_file, \n",
    "                        list_list_classes,\n",
    "                        balanced=False,\n",
    "                        apply_data_augmentation=False,\n",
    "                        batch_size=1, \n",
    "                        prefetch_buffer=None, \n",
    "                        shuffle=False,\n",
    "                        size=None):\n",
    "'''\n",
    "\n",
    "train_dataset, y_true_train = dt.create_dataset_new(TRAIN_FILE, \n",
    "                                                    DR_LEVELS_PER_CLASS, \n",
    "                                                    balanced=TRAINING_BALANCED, \n",
    "                                                    apply_data_augmentation=TRAINING_DATA_AUG, \n",
    "                                                    batch_size=TRAINING_BATCH_SIZE, \n",
    "                                                    prefetch_buffer=TRAINING_PREFETCH, \n",
    "                                                    one_hot_format=ONE_HOT_FORMAT,\n",
    "                                                    size=TRAINING_TAKE_SIZE)\n",
    "\n",
    "val_dataset, y_true_val = dt.create_dataset_new(VALIDATION_FILE, \n",
    "                                                DR_LEVELS_PER_CLASS, \n",
    "                                                balanced=VALIDATION_BALANCED,\n",
    "                                                apply_data_augmentation=VALIDATION_DATA_AUG,\n",
    "                                                batch_size=VALIDATION_BATCH_SIZE,\n",
    "                                                prefetch_buffer=VALIDATION_PREFETCH, \n",
    "                                                is_validation=True,\n",
    "                                                one_hot_format=ONE_HOT_FORMAT,\n",
    "                                                size=VALIDATION_TAKE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-opinion",
   "metadata": {},
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_true_train[:,0]), y=y_true_train[:,0])\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "print(d_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "reload(models)\n",
    "\n",
    "model = models.inceptionResNetV2.get_model(input_shape=(540,540,3), num_outputs=len(DR_LEVELS_PER_CLASS))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights if they were saved\n",
    "base_path = 'saved_weights/inceptionResNetV2/RMSProp/'\n",
    "save_path = base_path + 'model.h5'\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.mkdir(base_path)\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    model.load_weights(save_path)\n",
    "    print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ground truth for validation dataset\n",
    "np.save(base_path + 'ground_truth_val.npy', y_true_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "reload(metrics)\n",
    "\n",
    "classes_names = [''.join(list(map(str,class_))) for class_ in DR_LEVELS_PER_CLASS]\n",
    "\n",
    "print(classes_names)\n",
    "\n",
    "metric_AUC_0_1234 = metrics.Wrapper_AUC(classes=[1], original_dr_lvls=classes_names, is_one_hot=ONE_HOT_FORMAT)\n",
    "\n",
    "metric_Sp_at_95_Sens_0_1234 = metrics.Wrapper_SpecificityAtSensitivity(sensitivity=0.95, classes=[1], original_dr_lvls=classes_names, is_one_hot=ONE_HOT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbacks = [tf.keras.callbacks.ModelCheckpoint(base_path + 'best' + metric_AUC_0_1234.get_custom_name() + '.h5', \n",
    "                                             monitor='val_' + metric_AUC_0_1234.get_custom_name(),\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='max'),\n",
    "\n",
    "          tf.keras.callbacks.ModelCheckpoint(base_path + 'best' + metric_Sp_at_95_Sens_0_1234.get_custom_name() + '.h5', \n",
    "                                             monitor='val_' + metric_Sp_at_95_Sens_0_1234.get_custom_name(),\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='max'),\n",
    "          \n",
    "          tf.keras.callbacks.ModelCheckpoint(base_path + 'best_accuracy.h5',\n",
    "                                             monitor='val_accuracy',\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='max'),\n",
    "\n",
    "          tf.keras.callbacks.ModelCheckpoint(base_path + 'best_loss.h5',\n",
    "                                             monitor='val_loss',\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='min'),\n",
    "          \n",
    "          callbacks.Save_Training_Evolution(base_path + 'training_evolution.csv')\n",
    "]\n",
    "\n",
    "# cbacks = [tf.keras.callbacks.ModelCheckpoint(base_path + 'best_loss.h5',\n",
    "#                                              monitor='val_loss',\n",
    "#                                              save_best_only=True,\n",
    "#                                              save_weights_only=True,\n",
    "#                                              mode='min'),\n",
    "          \n",
    "#           callbacks.Save_Training_Evolution(base_path + 'training_evolution.csv')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From Voets' GitHub (Google replication 2018-19)\n",
    "# This is TF 1.X\n",
    "decay = 4e-5 # This value is the weight decay specified in their papers\n",
    "...\n",
    "train_op = tf.train.RMSPropOptimizer(\n",
    "    learning_rate=learning_rate, decay=decay) \\ # so, in TF 1.x, RMSProp has an argument called 'decay' for this purpose\n",
    "        .minimize(loss=mean_xentropy, global_step=global_step)\n",
    "\n",
    "### TF 1.x RMSProp description\n",
    "https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/RMSPropOptimizer\n",
    "\n",
    "Argument 'decay' -- Discounting factor for the history/coming gradient\n",
    "\n",
    "### TF 2.x RMSProp description\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop\n",
    "\n",
    "Argument 'rho' -- Discounting factor for the history/coming gradient. Defaults to 0.9.\n",
    "\n",
    "So, 'rho' should be the weight decay argument specified on Voets' code\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(metrics)\n",
    "\n",
    "validation_dir = base_path+'validation_outputs/'\n",
    "\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=4e-5, momentum=0.9, clipnorm=1.0), # There is no momentum reference in the papers\n",
    "              metrics=['accuracy',\n",
    "                       metric_AUC_0_1234, # DR levels: 0 vs 1,2,3,4\n",
    "                       metric_Sp_at_95_Sens_0_1234, # DR levels: 0 vs 1,2,3,4\n",
    "                       metrics.RunningValidation(path=validation_dir, n_columns=len(DR_LEVELS_PER_CLASS)) # THIS METRIC MUST BE INIZIALIZATED BEFORE EVERY TRAINING\n",
    "                      ]) \n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "# history = model.fit(train_dataset.take(50), epochs=num_epochs, validation_data=val_dataset.take(10), verbose=1, callbacks=cbacks, class_weight=d_class_weights)\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=1, callbacks=cbacks, class_weight=d_class_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit6c4dec2e42734bc298ce0c3bfcfccb75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
