{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fuzzy-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.custom_callbacks as callbacks\n",
    "import lib.custom_metrics as metrics\n",
    "import lib.evaluation as ev\n",
    "import lib.plotting as plot\n",
    "import lib.models as models\n",
    "import lib.dataset as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imgaug.augmenters as iaa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aquatic-therapist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heated-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_LEVELS_PER_CLASS = [[0], [1,2,3,4]]\n",
    "\n",
    "IMAGE_SIZE = (540, 540, 3)\n",
    "\n",
    "# Specify dataset files\n",
    "TRAIN_FILE = 'DATASET-TRAIN-80.csv'\n",
    "VALIDATION_FILE = 'DATASET-VALIDATION-10_BALANCED.csv'\n",
    "\n",
    "ONE_HOT_FORMAT = False\n",
    "\n",
    "TRAINING_BATCH_SIZE = 12\n",
    "TRAINING_DATA_AUG = True\n",
    "TRAINING_BALANCED = False\n",
    "TRAINING_PREFETCH = 20\n",
    "TRAINING_TAKE_SIZE = None\n",
    "\n",
    "VALIDATION_BATCH_SIZE = 12\n",
    "VALIDATION_DATA_AUG = False\n",
    "VALIDATION_BALANCED = False\n",
    "VALIDATION_PREFETCH = 1\n",
    "VALIDATION_TAKE_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerous-bookmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se aplica data aug\n"
     ]
    }
   ],
   "source": [
    "reload(dt)\n",
    "\n",
    "'''\n",
    "def create_dataset_new(csv_file, \n",
    "                        list_list_classes,\n",
    "                        balanced=False,\n",
    "                        apply_data_augmentation=False,\n",
    "                        batch_size=1, \n",
    "                        prefetch_buffer=None, \n",
    "                        shuffle=False,\n",
    "                        size=None):\n",
    "'''\n",
    "\n",
    "train_dataset, y_true_train = dt.create_dataset_new(TRAIN_FILE, \n",
    "                                                    DR_LEVELS_PER_CLASS, \n",
    "                                                    balanced=TRAINING_BALANCED, \n",
    "                                                    apply_data_augmentation=TRAINING_DATA_AUG, \n",
    "                                                    batch_size=TRAINING_BATCH_SIZE, \n",
    "                                                    prefetch_buffer=TRAINING_PREFETCH, \n",
    "                                                    one_hot_format=ONE_HOT_FORMAT,\n",
    "                                                    size=TRAINING_TAKE_SIZE)\n",
    "\n",
    "val_dataset, y_true_val = dt.create_dataset_new(VALIDATION_FILE, \n",
    "                                                DR_LEVELS_PER_CLASS, \n",
    "                                                balanced=VALIDATION_BALANCED,\n",
    "                                                apply_data_augmentation=VALIDATION_DATA_AUG,\n",
    "                                                batch_size=VALIDATION_BATCH_SIZE,\n",
    "                                                prefetch_buffer=VALIDATION_PREFETCH, \n",
    "                                                is_validation=True,\n",
    "                                                one_hot_format=ONE_HOT_FORMAT,\n",
    "                                                size=VALIDATION_TAKE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-electric",
   "metadata": {},
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comic-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6740610690645092, 1: 1.9362775165269543}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_true_train[:,0]), y=y_true_train[:,0])\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "print(d_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "binding-adapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94674944/94668760 [==============================] - 5s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50v2 (Functional)      (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,568,898\n",
      "Trainable params: 23,523,458\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "reload(models)\n",
    "\n",
    "model = models.inceptionResNetV2.get_model(input_shape=(540,540,3), num_outputs=len(DR_LEVELS_PER_CLASS))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aerial-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights if they were saved\n",
    "base_path = 'saved_weights/inceptionResNetV2/SGD_3/'\n",
    "save_path = base_path + 'model.h5'\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.mkdir(base_path)\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    model.load_weights(save_path)\n",
    "    print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proper-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ground truth for validation dataset\n",
    "np.save(base_path + 'ground_truth_val.npy', y_true_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collected-lancaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1234']\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "reload(metrics)\n",
    "\n",
    "classes_names = [''.join(list(map(str,class_))) for class_ in DR_LEVELS_PER_CLASS]\n",
    "\n",
    "print(classes_names)\n",
    "\n",
    "metric_AUC_0_1234 = metrics.Wrapper_AUC(classes=[1], original_dr_lvls=classes_names, is_one_hot=ONE_HOT_FORMAT)\n",
    "\n",
    "metric_Sp_at_95_Sens_0_1234 = metrics.Wrapper_SpecificityAtSensitivity(sensitivity=0.95, classes=[1], original_dr_lvls=classes_names, is_one_hot=ONE_HOT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suited-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbacks = [tf.keras.callbacks.ModelCheckpoint(base_path + 'best' + metric_AUC_0_1234.get_custom_name() + '.h5', \n",
    "                                             monitor='val_' + metric_AUC_0_1234.get_custom_name(),\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='max'),\n",
    "\n",
    "          tf.keras.callbacks.ModelCheckpoint(base_path + 'best' + metric_Sp_at_95_Sens_0_1234.get_custom_name() + '.h5', \n",
    "                                             monitor='val_' + metric_Sp_at_95_Sens_0_1234.get_custom_name(),\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='max'),\n",
    "          \n",
    "          tf.keras.callbacks.ModelCheckpoint(base_path + 'best_accuracy.h5',\n",
    "                                             monitor='val_accuracy',\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='max'),\n",
    "\n",
    "          tf.keras.callbacks.ModelCheckpoint(base_path + 'best_loss.h5',\n",
    "                                             monitor='val_loss',\n",
    "                                             save_best_only=True,\n",
    "                                             save_weights_only=True,\n",
    "                                             mode='min'),\n",
    "          \n",
    "          callbacks.Save_Training_Evolution(base_path + 'training_evolution.csv')\n",
    "]\n",
    "\n",
    "# cbacks = [tf.keras.callbacks.ModelCheckpoint(base_path + 'best_loss.h5',\n",
    "#                                              monitor='val_loss',\n",
    "#                                              save_best_only=True,\n",
    "#                                              save_weights_only=True,\n",
    "#                                              mode='min'),\n",
    "          \n",
    "#           callbacks.Save_Training_Evolution(base_path + 'training_evolution.csv')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "2805/9471 [=======>......................] - ETA: 16:02 - loss: 0.6673 - accuracy: 0.6812 - AUC_DRlvls_0_1234: 0.6290 - Sp_at_95_sens_DRlvls_0_1234: 0.1022 - RunningValidation: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "reload(metrics)\n",
    "\n",
    "validation_dir = base_path+'validation_outputs/'\n",
    "\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, clipnorm=1.0), \n",
    "              metrics=['accuracy',\n",
    "                       metric_AUC_0_1234, # DR levels: 0 vs 1,2,3,4\n",
    "                       metric_Sp_at_95_Sens_0_1234, # DR levels: 0 vs 1,2,3,4\n",
    "                       metrics.RunningValidation(path=validation_dir, n_columns=len(DR_LEVELS_PER_CLASS)) # THIS METRIC MUST BE INIZIALIZATED BEFORE EVERY TRAINING\n",
    "                      ]) \n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "# history = model.fit(train_dataset.take(50), epochs=num_epochs, validation_data=val_dataset.take(10), verbose=1, callbacks=cbacks, class_weight=d_class_weights)\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, verbose=1, callbacks=cbacks, class_weight=d_class_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit6c4dec2e42734bc298ce0c3bfcfccb75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
